{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "import gym\n",
    "!apt install cmake libopenmpi-dev zlib1g-dev\n",
    "#installing dependencies otherwise pip installation will thow error\n",
    "!pip install stable-baselines \n",
    "from stable_baselines.common.vec_env import DummyVecEnv \n",
    "from stable_baselines.deepq.policies import MlpPolicy,CnnPolicy \n",
    "from stable_baselines import DQN\n",
    "!apt-get install python-opengl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "env = gym.make('MsPacman-v0')\n",
    "#learning parameters\n",
    "y = .95 #gamma\n",
    "e = 0.1 #random selection epsilion\n",
    "num_episodes = 2000\n",
    "RANDOM_THRESHOLD = 1000 #minimum number of frames to choose random actions for\n",
    "memory_size = 1000\n",
    "train_batch_size = 64\n",
    "env_features = 210*160*3\n",
    "\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.1, shape=[length]))\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def new_conv_layer(input, #the previous layer\n",
    "                   num_input_channels, #channels in the previous layer\n",
    "                   filter_size, #width and height of each filter\n",
    "                   num_filters, #number of filters\n",
    "                   max_pooled=True): #use 2x2 max-pooling\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    weights = new_weights(shape=shape)\n",
    "    biases  = new_biases(length=num_filters)\n",
    "    layer = conv2d(input, weights)\n",
    "    layer += biases\n",
    "    if max_pooled:\n",
    "        layer = max_pool_2x2(layer)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "\n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features\n",
    "\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "class QNet():\n",
    "    def __init__(self, trainable=False):\n",
    "        self.env_obs = tf.placeholder(shape=[None, 210, 160, 3], dtype=tf.float32)\n",
    "        self.layer1 = new_conv_layer(input=self.env_obs, num_input_channels=3,filter_size=5,num_filters=32,max_pooled=True)\n",
    "        self.layer2 = new_conv_layer(input=self.layer1, num_input_channels=32,filter_size=5,num_filters=32,max_pooled=True)\n",
    "        self.layer3 = new_conv_layer(input=self.layer2, num_input_channels=32,filter_size=5,num_filters=64,max_pooled=True)\n",
    "        self.l_flat, self.num_features = flatten_layer(self.layer3)\n",
    "        self.fc_1 = new_fc_layer(self.l_flat, self.num_features, 128)\n",
    "        self.fc_2 = new_fc_layer(self.fc_1, 128, env.action_space.n, False)\n",
    "        self.q_out = tf.nn.softmax(self.fc_2)\n",
    "        self.predict = tf.argmax(self.q_out)\n",
    "        if trainable:\n",
    "            self.q_next = tf.placeholder(shape=[None, env.action_space.n], dtype=tf.float32)\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_next, self.q_out))\n",
    "            self.train_op = tf.train.RMSPropOptimizer(0.05).minimize(self.loss)\n",
    "\n",
    "#build a table to store previous game states\n",
    "table = np.zeros((memory_size, 210 * 160 * 3 * 2 + 2))\n",
    "\n",
    "def store_transition(state, action, reward, observation):\n",
    "    '''stores a transition for training the model'''\n",
    "    if 'table_idx' not in globals():\n",
    "        global table_idx\n",
    "        table_idx = 0\n",
    "    state = np.reshape(state, env_features)\n",
    "    observation = np.reshape(observation, env_features)\n",
    "    transition = np.hstack((state, [action, reward], observation))\n",
    "    index = table_idx % memory_size #overwrite old values\n",
    "    table[index, :] = transition\n",
    "    table_idx += 1\n",
    "\n",
    "mainQN = QNet(True)\n",
    "targetQN = QNet()\n",
    "with tf.Session() as sess:\n",
    "    frames = 0\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    f_reward = 0\n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            frames += 1\n",
    "            if frames < RANDOM_THRESHOLD or np.random.rand(1) < e:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = sess.run([mainQN.predict],feed_dict={mainQN.env_obs:[state]})\n",
    "                action = np.argmax(action)\n",
    "            #Get new state and reward from environment\n",
    "            observation,reward,done,_ = env.step(action)\n",
    "            store_transition(state, action, reward, observation)\n",
    "            if frames >= RANDOM_THRESHOLD and frames % 4 == 0:\n",
    "                if table_idx > memory_size:\n",
    "                    sample_index = np.random.choice(memory_size, size=train_batch_size)\n",
    "                else:\n",
    "                    sample_index = np.random.choice(table_idx, size=train_batch_size)\n",
    "                batch = table[sample_index, :]\n",
    "                _observations = batch[:, :env_features]\n",
    "                _observations = np.reshape(_observations, (-1, 210, 160, 3))\n",
    "                _observations_next = batch[:, -env_features:]\n",
    "                _observations_next = np.reshape(_observations_next, (-1, 210, 160, 3))\n",
    "                q_next = sess.run(mainQN.q_out, feed_dict={mainQN.env_obs: _observations_next})\n",
    "                q_eval_next = sess.run(targetQN.q_out, feed_dict={targetQN.env_obs: _observations_next})\n",
    "                q_eval = sess.run(mainQN.q_out, feed_dict={mainQN.env_obs: _observations})\n",
    "                q_target = q_eval.copy()\n",
    "                batch_index = np.arange(train_batch_size, dtype=np.int32)\n",
    "                eval_act_index = batch[:, env_features].astype(int)\n",
    "                _reward = batch[:, env_features + 1]\n",
    "                max_action = np.argmax(q_eval_next, axis=1)\n",
    "                next_selected = q_next[batch_index, max_action]\n",
    "                q_target[batch_index, eval_act_index] = _reward + y * next_selected\n",
    "                sess.run(mainQN.train_op, feed_dict={mainQN.env_obs: _observations, mainQN.q_next: q_target})\n",
    "            state = observation\n",
    "            f_reward += reward\n",
    "            if done:\n",
    "                saver.save(sess, 'mspacman/model')\n",
    "                print('mean_reward: ',f_reward/(i+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
